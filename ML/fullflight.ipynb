{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da0f0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import whisper\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(name):\n",
    "    return \"../uploads/\" + name\n",
    "\n",
    "def plot_time(x, y, xlabel, ylabel, title):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "frame_dir = \"frames\"\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "def extract_frames(name):\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-i\", name,\n",
    "        \"-vf\", \"fps=10\",\n",
    "        f\"{frame_dir}/{os.path.basename(name)}_frame_%04d.png\"\n",
    "    ])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def save_audio_rms_to_csv(name, csv_filename=None, plot=False):\n",
    "    y, sr = librosa.load(get_video(name), sr=None)\n",
    "    frame_length = int(sr * 0.1)  # 100ms frames\n",
    "    hop_length = frame_length\n",
    "\n",
    "    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    times = librosa.frames_to_time(range(len(rms)), sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    if plot:\n",
    "        plot_time(times, 20 * np.log10(rms), xlabel=\"Time (s)\", ylabel=\"Volume (dB)\", title=\"Audio Levels Over Time\")\n",
    "    \n",
    "    if csv_filename is not None:\n",
    "        with open(csv_filename, mode='w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Time (s)\", \"RMS\"])\n",
    "            for t, r in zip(times, rms):\n",
    "                writer.writerow([t, r])\n",
    "\n",
    "\n",
    "# Save frame brightness to CSV only if a filename is explicitly provided\n",
    "def save_frame_brightness_to_csv(name, csv_filename=None):\n",
    "    brightness_data = []\n",
    "    if csv_filename is None:\n",
    "        for fname in sorted(os.listdir(frame_dir)):\n",
    "            if fname.endswith(\".png\"):\n",
    "                img = cv2.imread(os.path.join(frame_dir, fname))\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                brightness = np.mean(gray)\n",
    "                brightness_data.append(brightness)\n",
    "                \n",
    "            timestamps = [i * 0.1 for i in range(len(brightness_data))]\n",
    "\n",
    "        return brightness_data, timestamps\n",
    "\n",
    "    for fname in sorted(os.listdir(frame_dir)):\n",
    "        if fname.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(frame_dir, fname))\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            brightness = np.mean(gray)\n",
    "            brightness_data.append(brightness)\n",
    "\n",
    "    timestamps = [i * 0.1 for i in range(len(brightness_data))]\n",
    "    with open(csv_filename, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Time (s)\", \"Brightness\"])\n",
    "        for t, b in zip(timestamps, brightness_data):\n",
    "            writer.writerow([t, b])\n",
    "\n",
    "# Example usage:\n",
    "extract_frames(\"[Kayoanime] Solo Leveling - S02E06.mkv\")\n",
    "save_audio_rms_to_csv(\"[Kayoanime] Solo Leveling - S02E06.mkv\")\n",
    "save_frame_brightness_to_csv(\"[Kayoanime] Solo Leveling - S02E06.mkv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialouge detection and anger detection\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "input_video = \"input.mkv\"\n",
    "output_audio = \"audio.wav\"\n",
    "anger_threshold = 0.3\n",
    "merge_gap = 3\n",
    "csv_filename = \"angry_sections.csv\"\n",
    "\n",
    "# Extract audio: 16kHz mono WAV (required for Whisper)\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-y\", \"-i\", input_video,\n",
    "    \"-vn\",                      # No video\n",
    "    \"-acodec\", \"pcm_s16le\",     # 16-bit PCM audio\n",
    "    \"-ar\", \"16000\",             # 16kHz sampling rate\n",
    "    \"-ac\", \"1\",                 # Mono\n",
    "    output_audio\n",
    "])\n",
    "\n",
    "# Whipser model\n",
    "model = whisper.load_model(\"base\")  # change model size when tuinng\n",
    "result = model.transcribe(output_audio)\n",
    "\n",
    "print(\"Loading emotion classifier...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "labels = ['anger', 'joy', 'optimism', 'sadness']\n",
    "\n",
    "# Angry per segment\n",
    "angry_segments = []\n",
    "\n",
    "for seg in result[\"segments\"]:\n",
    "\n",
    "    text = seg[\"text\"]\n",
    "    start = seg[\"start\"]\n",
    "    end = seg[\"end\"]\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1)[0]\n",
    "\n",
    "    anger_score = probs[labels.index(\"anger\")].item()\n",
    "\n",
    "    if anger_score > anger_threshold:\n",
    "        angry_segments.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": text.strip(),\n",
    "            \"anger_score\": anger_score\n",
    "        })\n",
    "\n",
    "# Group angry segmenet\n",
    "grouped_angry_sections = []\n",
    "\n",
    "if angry_segments:\n",
    "    current_group = [angry_segments[0]]\n",
    "    for seg in angry_segments[1:]:\n",
    "        prev = current_group[-1]\n",
    "        if seg['start'] - prev['end'] <= merge_gap:\n",
    "            current_group.append(seg)\n",
    "        else:\n",
    "            grouped_angry_sections.append(current_group)\n",
    "            current_group = [seg]\n",
    "    grouped_angry_sections.append(current_group)\n",
    "\n",
    "# Rank groups and print\n",
    "def sec_to_mmss(sec):\n",
    "    return str(datetime.timedelta(seconds=int(sec)))\n",
    "\n",
    "ranked_sections = []\n",
    "\n",
    "for group in grouped_angry_sections:\n",
    "    start = group[0]['start']\n",
    "    end = group[-1]['end']\n",
    "    avg_anger = sum(s['anger_score'] for s in group) / len(group)\n",
    "    ranked_sections.append({\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"avg_anger\": avg_anger,\n",
    "        \"segments\": group\n",
    "    })\n",
    "\n",
    "# Sort by avg anger desc\n",
    "ranked_sections.sort(key=lambda x: x[\"avg_anger\"], reverse=True)\n",
    "\n",
    "# Print full grouped anger sections\n",
    "print(\"\\nAngriest Sections in the Video:\\n\")\n",
    "for i, section in enumerate(ranked_sections[:5]):  # Get top 5 section, hopefully enough?\n",
    "    print(f\"🔴 Section {i+1}\")\n",
    "    print(f\"[{sec_to_mmss(section['start'])} → {sec_to_mmss(section['end'])}] | Avg Anger Score: {section['avg_anger']:.2f}\\n\")\n",
    "\n",
    "    for seg in section[\"segments\"]:\n",
    "        print(f\"  🕒 [{sec_to_mmss(seg['start'])} → {sec_to_mmss(seg['end'])}] | Anger: {seg['anger_score']:.2f}\")\n",
    "        print(f\"  🗣️  {seg['text']}\\n\")\n",
    "\n",
    "    print(\"------------------------------------------------------------\\n\")\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        \"Section Number\",\n",
    "        \"Section Start\",\n",
    "        \"Section End\",\n",
    "        \"Section Avg Anger\",\n",
    "        \"Segment Start\",\n",
    "        \"Segment End\",\n",
    "        \"Segment Anger Score\",\n",
    "        \"Segment Text\"\n",
    "    ])\n",
    "\n",
    "    for i, section in enumerate(ranked_sections[:5], start=1):\n",
    "        section_start = sec_to_mmss(section[\"start\"])\n",
    "        section_end = sec_to_mmss(section[\"end\"])\n",
    "        avg_anger = f\"{section['avg_anger']:.2f}\"\n",
    "\n",
    "        for seg in section[\"segments\"]:\n",
    "            seg_start = sec_to_mmss(seg[\"start\"])\n",
    "            seg_end = sec_to_mmss(seg[\"end\"])\n",
    "            seg_anger = f\"{seg['anger_score']:.2f}\"\n",
    "            seg_text = seg[\"text\"].replace('\\n', ' ').strip()\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                section_start,\n",
    "                section_end,\n",
    "                avg_anger,\n",
    "                seg_start,\n",
    "                seg_end,\n",
    "                seg_anger,\n",
    "                seg_text\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge dialogue, audio RMS, and frame brightness CSVs by timestamp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "dialogue_df = pd.read_csv(\"angry_sections.csv\")  # or your dialogue csv\n",
    "audio_df = pd.read_csv(\"audio_rms.csv\")\n",
    "brightness_df = pd.read_csv(\"frame_brightness.csv\")\n",
    "\n",
    "# Prepare audio and brightness for merging (round timestamps for alignment)\n",
    "audio_df[\"Time (s)\"] = audio_df[\"Time (s)\"].round(1)\n",
    "brightness_df[\"Time (s)\"] = brightness_df[\"Time (s)\"].round(1)\n",
    "\n",
    "# Merge audio and brightness on timestamp \n",
    "merged_df = pd.merge(audio_df, brightness_df, on=\"Time (s)\", how=\"outer\")\n",
    "\n",
    "# If dialogue has timestamps, merge them as well (example assumes Segment Start/End in seconds)\n",
    "# You may need to adjust this part based on your dialogue CSV format\n",
    "if \"Segment Start\" in dialogue_df.columns:\n",
    "    # Convert Segment Start to seconds if needed\n",
    "    def mmss_to_sec(mmss):\n",
    "        try:\n",
    "            parts = str(mmss).split(':')\n",
    "            return int(parts[0]) * 60 + int(parts[1])\n",
    "        except:\n",
    "            return None\n",
    "    dialogue_df[\"Start_sec\"] = dialogue_df[\"Segment Start\"].apply(mmss_to_sec)\n",
    "    # Round to nearest 0.1s for alignment\n",
    "    dialogue_df[\"Start_sec\"] = dialogue_df[\"Start_sec\"].round(1)\n",
    "    # Merge dialogue info into merged_df\n",
    "    merged_df = pd.merge(merged_df, dialogue_df, left_on=\"Time (s)\", right_on=\"Start_sec\", how=\"left\")\n",
    "\n",
    "# Save merged CSV\n",
    "merged_df.to_csv(\"merged_features.csv\", index=False)\n",
    "print(\"Merged CSV saved as merged_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load merged features\n",
    "df = pd.read_csv(\"merged_features.csv\")\n",
    "\n",
    "# Plot Audio RMS and Brightness over time\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df[\"Time (s)\"], df[\"RMS\"], label=\"Audio RMS\")\n",
    "plt.plot(df[\"Time (s)\"], df[\"Brightness\"], label=\"Frame Brightness\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Audio RMS and Frame Brightness Over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# If you have anger scores or dialogue, plot those too\n",
    "if \"Segment Anger Score\" in df.columns:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(df[\"Time (s)\"], df[\"Segment Anger Score\"], label=\"Anger Score\", color='red')\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Anger Score\")\n",
    "    plt.title(\"Dialogue Anger Score Over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
